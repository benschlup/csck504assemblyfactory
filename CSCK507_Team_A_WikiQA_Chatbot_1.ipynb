{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyOKtS+pFosqxj0CdTWDiiLQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck504assemblyfactory/blob/main/CSCK507_Team_A_WikiQA_Chatbot_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 1: LSTM without Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "\n",
        "Important note: \n",
        "The dataset includes incorrect answers, labelled accordingly. Learning from these can be switches on/off (see below).\n",
        "\n",
        "In a real setting, it would be sensible to add a concept called \"answer triggering\" and exclude learning from incorrect answers. Answer triggering  first assesses a question to qualify if the model may deliver a sensible answer - otherwise let the person know that the bot does not know. Ref: https://ieeexplore.ieee.org/document/8079800\n",
        "\n",
        "Additional interesting materials to review, and potentially reference:\n",
        "Khin, N.N., Soe, K.M., 2020. Question Answering based University Chatbot using Sequence to Sequence Model, in: .. doi:10.1109/o-cocosda50338.2020.9295021\n",
        "\n"
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Backlog:\n",
        "* Strip whitespace at beginning and end of normalized questions and answers\n",
        "* Add drop-out layer\n",
        "\n",
        "For future study, i.e. to be mentioned in report\n",
        "* Check if lemmatizing on question side improves performance\n",
        "* Check if word embedding (e.g. using Word2Vec or GloVe) on question (i.e. input side) improves performance (beware of out-of-vocab)\n",
        "---"
      ],
      "metadata": {
        "id": "AHuZjEDChQ_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuration"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = 20    # set this to None if no limit on question length\n",
        "max_answer_tokens = 50      # set this to None if no limit on answer length"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Fq0L8soItfbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#from gensim.models import Word2Vec\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15f2dce5-8942-49b6-ab5f-7fb04bef7da7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 18:01:17 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "65deac14-3129-4805-d556-c587aa73c4d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7fae52752510>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "a1caf9a7-4ebb-4219-81ed-c5cb0f6846a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower()+' _STOP_' for s in df['Sentence']]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "37fef318-19c3-4e2d-cab2-5f17a9d31faf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1ab443-a16d-435c-8108-ee1d73e893c6",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2181\n",
            "Net validation dataset size: 108\n",
            "Net test dataset size: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {train_decoder_output_data.shape}')\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(vocab_size, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(vocab_size, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(vocab_size, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "08335e03-d10a-4ed2-c9c5-3abfa556f502"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2181, 21)\n",
            "Decoder input data shape: (2181, 52)\n",
            "Decoder output data shape: (2181, 52, 6001)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 200)    1200200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    1200200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 200),        320800      ['embedding[0][0]']              \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'lstm[0][1]',                   \n",
            "                                 (None, 200)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 6001)   1206201     ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,248,201\n",
            "Trainable params: 4,248,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=50, epochs=200)\n",
        "\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "08cc1af6-7344-43a5-bc1d-6d161d8faf93"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 15s 113ms/step - loss: 2.1998 - val_loss: 2.0184\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.9929 - val_loss: 1.9598\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.9283 - val_loss: 1.9295\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.8765 - val_loss: 1.9014\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.8290 - val_loss: 1.8669\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.7864 - val_loss: 1.8422\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.7496 - val_loss: 1.8263\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.7174 - val_loss: 1.8150\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.6876 - val_loss: 1.8032\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.6581 - val_loss: 1.7953\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.6286 - val_loss: 1.7856\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.5986 - val_loss: 1.7749\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.5691 - val_loss: 1.7649\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.5382 - val_loss: 1.7578\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 1.5082 - val_loss: 1.7498\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.4779 - val_loss: 1.7454\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.4479 - val_loss: 1.7382\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.4180 - val_loss: 1.7335\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.3882 - val_loss: 1.7328\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.3597 - val_loss: 1.7239\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.3305 - val_loss: 1.7187\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.3009 - val_loss: 1.7199\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.2725 - val_loss: 1.7156\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.2440 - val_loss: 1.7139\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.2154 - val_loss: 1.7101\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.1866 - val_loss: 1.7102\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 1.1595 - val_loss: 1.7085\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.1317 - val_loss: 1.7055\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.1039 - val_loss: 1.7052\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.0776 - val_loss: 1.7009\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.0500 - val_loss: 1.7029\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 1.0236 - val_loss: 1.7063\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.9976 - val_loss: 1.7021\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.9720 - val_loss: 1.7079\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.9456 - val_loss: 1.7037\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.9206 - val_loss: 1.7042\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.8951 - val_loss: 1.7134\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.8712 - val_loss: 1.7093\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.8470 - val_loss: 1.7095\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.8235 - val_loss: 1.7077\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.7991 - val_loss: 1.7131\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.7755 - val_loss: 1.7187\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.7534 - val_loss: 1.7195\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.7312 - val_loss: 1.7198\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.7088 - val_loss: 1.7217\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6872 - val_loss: 1.7244\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.6670 - val_loss: 1.7268\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.6456 - val_loss: 1.7344\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.6256 - val_loss: 1.7342\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6057 - val_loss: 1.7332\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.5858 - val_loss: 1.7433\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.5676 - val_loss: 1.7457\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.5485 - val_loss: 1.7404\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5298 - val_loss: 1.7661\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5129 - val_loss: 1.7524\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.4957 - val_loss: 1.7603\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.4785 - val_loss: 1.7684\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.4622 - val_loss: 1.7655\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4459 - val_loss: 1.7752\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4305 - val_loss: 1.7748\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.4153 - val_loss: 1.7822\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4003 - val_loss: 1.7918\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3858 - val_loss: 1.7890\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3715 - val_loss: 1.8010\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3578 - val_loss: 1.8068\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.3446 - val_loss: 1.8057\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3324 - val_loss: 1.8205\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.3195 - val_loss: 1.8204\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.3075 - val_loss: 1.8300\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.2954 - val_loss: 1.8250\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2833 - val_loss: 1.8363\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2723 - val_loss: 1.8419\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2622 - val_loss: 1.8467\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2509 - val_loss: 1.8454\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2410 - val_loss: 1.8614\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2314 - val_loss: 1.8674\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2223 - val_loss: 1.8673\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2127 - val_loss: 1.8769\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2040 - val_loss: 1.8917\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1961 - val_loss: 1.8967\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1879 - val_loss: 1.8986\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1801 - val_loss: 1.9064\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.1729 - val_loss: 1.9116\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1649 - val_loss: 1.9153\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1575 - val_loss: 1.9286\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1515 - val_loss: 1.9250\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.1456 - val_loss: 1.9516\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.1395 - val_loss: 1.9302\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1331 - val_loss: 1.9533\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1278 - val_loss: 1.9453\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1224 - val_loss: 1.9538\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1166 - val_loss: 1.9684\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1117 - val_loss: 1.9685\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1077 - val_loss: 1.9843\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1032 - val_loss: 1.9723\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0988 - val_loss: 1.9933\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0948 - val_loss: 2.0007\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0910 - val_loss: 1.9996\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0874 - val_loss: 2.0162\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0841 - val_loss: 2.0193\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0806 - val_loss: 2.0270\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0775 - val_loss: 2.0227\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0747 - val_loss: 2.0511\n",
            "Epoch 104/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0720 - val_loss: 2.0368\n",
            "Epoch 105/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0693 - val_loss: 2.0479\n",
            "Epoch 106/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0667 - val_loss: 2.0482\n",
            "Epoch 107/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0647 - val_loss: 2.0673\n",
            "Epoch 108/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0625 - val_loss: 2.0626\n",
            "Epoch 109/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0603 - val_loss: 2.0849\n",
            "Epoch 110/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0585 - val_loss: 2.0751\n",
            "Epoch 111/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0566 - val_loss: 2.0860\n",
            "Epoch 112/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0548 - val_loss: 2.1187\n",
            "Epoch 113/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0531 - val_loss: 2.1002\n",
            "Epoch 114/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0519 - val_loss: 2.1038\n",
            "Epoch 115/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0505 - val_loss: 2.1315\n",
            "Epoch 116/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0490 - val_loss: 2.1217\n",
            "Epoch 117/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0476 - val_loss: 2.1268\n",
            "Epoch 118/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0469 - val_loss: 2.1333\n",
            "Epoch 119/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0454 - val_loss: 2.1417\n",
            "Epoch 120/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0445 - val_loss: 2.1359\n",
            "Epoch 121/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0435 - val_loss: 2.1391\n",
            "Epoch 122/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0428 - val_loss: 2.1490\n",
            "Epoch 123/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0415 - val_loss: 2.1538\n",
            "Epoch 124/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0409 - val_loss: 2.1571\n",
            "Epoch 125/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0401 - val_loss: 2.1657\n",
            "Epoch 126/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0395 - val_loss: 2.1804\n",
            "Epoch 127/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0386 - val_loss: 2.1631\n",
            "Epoch 128/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0382 - val_loss: 2.1907\n",
            "Epoch 129/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0377 - val_loss: 2.1914\n",
            "Epoch 130/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0369 - val_loss: 2.2001\n",
            "Epoch 131/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0365 - val_loss: 2.2047\n",
            "Epoch 132/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0360 - val_loss: 2.2083\n",
            "Epoch 133/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0354 - val_loss: 2.2136\n",
            "Epoch 134/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0350 - val_loss: 2.2381\n",
            "Epoch 135/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0347 - val_loss: 2.2191\n",
            "Epoch 136/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0345 - val_loss: 2.2161\n",
            "Epoch 137/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0340 - val_loss: 2.2297\n",
            "Epoch 138/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0338 - val_loss: 2.2388\n",
            "Epoch 139/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0333 - val_loss: 2.2535\n",
            "Epoch 140/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0332 - val_loss: 2.2398\n",
            "Epoch 141/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0328 - val_loss: 2.2497\n",
            "Epoch 142/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0323 - val_loss: 2.2560\n",
            "Epoch 143/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0322 - val_loss: 2.2651\n",
            "Epoch 144/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0320 - val_loss: 2.2680\n",
            "Epoch 145/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0316 - val_loss: 2.2820\n",
            "Epoch 146/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0315 - val_loss: 2.2813\n",
            "Epoch 147/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0312 - val_loss: 2.2730\n",
            "Epoch 148/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0311 - val_loss: 2.2991\n",
            "Epoch 149/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.0309 - val_loss: 2.2753\n",
            "Epoch 150/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0307 - val_loss: 2.2929\n",
            "Epoch 151/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0304 - val_loss: 2.3051\n",
            "Epoch 152/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0302 - val_loss: 2.3085\n",
            "Epoch 153/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0301 - val_loss: 2.3143\n",
            "Epoch 154/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0301 - val_loss: 2.3114\n",
            "Epoch 155/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0297 - val_loss: 2.3305\n",
            "Epoch 156/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0296 - val_loss: 2.3207\n",
            "Epoch 157/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.0294 - val_loss: 2.3403\n",
            "Epoch 158/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0294 - val_loss: 2.3152\n",
            "Epoch 159/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.0292 - val_loss: 2.3254\n",
            "Epoch 160/200\n",
            "44/44 [==============================] - 2s 40ms/step - loss: 0.0292 - val_loss: 2.3346\n",
            "Epoch 161/200\n",
            "44/44 [==============================] - 2s 40ms/step - loss: 0.0290 - val_loss: 2.3315\n",
            "Epoch 162/200\n",
            "44/44 [==============================] - 2s 40ms/step - loss: 0.0288 - val_loss: 2.3379\n",
            "Epoch 163/200\n",
            "44/44 [==============================] - 2s 39ms/step - loss: 0.0288 - val_loss: 2.3437\n",
            "Epoch 164/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0287 - val_loss: 2.3690\n",
            "Epoch 165/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0285 - val_loss: 2.3657\n",
            "Epoch 166/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0285 - val_loss: 2.3792\n",
            "Epoch 167/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0284 - val_loss: 2.3675\n",
            "Epoch 168/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0282 - val_loss: 2.3751\n",
            "Epoch 169/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0282 - val_loss: 2.3685\n",
            "Epoch 170/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0281 - val_loss: 2.3872\n",
            "Epoch 171/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0280 - val_loss: 2.3954\n",
            "Epoch 172/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0280 - val_loss: 2.3823\n",
            "Epoch 173/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0279 - val_loss: 2.4131\n",
            "Epoch 174/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0279 - val_loss: 2.4044\n",
            "Epoch 175/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0277 - val_loss: 2.4071\n",
            "Epoch 176/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0278 - val_loss: 2.4010\n",
            "Epoch 177/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0276 - val_loss: 2.4042\n",
            "Epoch 178/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0276 - val_loss: 2.4258\n",
            "Epoch 179/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0275 - val_loss: 2.4133\n",
            "Epoch 180/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0274 - val_loss: 2.4341\n",
            "Epoch 181/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0274 - val_loss: 2.4497\n",
            "Epoch 182/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0273 - val_loss: 2.4270\n",
            "Epoch 183/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0272 - val_loss: 2.4373\n",
            "Epoch 184/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0272 - val_loss: 2.4349\n",
            "Epoch 185/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0272 - val_loss: 2.4406\n",
            "Epoch 186/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0272 - val_loss: 2.4477\n",
            "Epoch 187/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0271 - val_loss: 2.4482\n",
            "Epoch 188/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0271 - val_loss: 2.4588\n",
            "Epoch 189/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0270 - val_loss: 2.4544\n",
            "Epoch 190/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0269 - val_loss: 2.4585\n",
            "Epoch 191/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0267 - val_loss: 2.4668\n",
            "Epoch 192/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0269 - val_loss: 2.4550\n",
            "Epoch 193/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0267 - val_loss: 2.4697\n",
            "Epoch 194/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0268 - val_loss: 2.4923\n",
            "Epoch 195/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0267 - val_loss: 2.4768\n",
            "Epoch 196/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0268 - val_loss: 2.4798\n",
            "Epoch 197/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0268 - val_loss: 2.4661\n",
            "Epoch 198/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0266 - val_loss: 2.4871\n",
            "Epoch 199/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0266 - val_loss: 2.4898\n",
            "Epoch 200/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0266 - val_loss: 2.5019\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae51ee5250>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n",
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    print('Inference encoder:')\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "def str_to_tokens(sentence):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sentence).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "af379bed-8311-44c9-ec7b-21e5e34fa14f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    1200200     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " input_4 (InputLayer)           [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 200),  320800      ['embedding_1[0][0]',            \n",
            "                                 (None, 200),                     'input_3[0][0]',                \n",
            "                                 (None, 200)]                     'input_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 6001)   1206201     ['lstm_1[1][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,727,201\n",
            "Trainable params: 2,727,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 200)         1200200   \n",
            "                                                                 \n",
            " lstm (LSTM)                 [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,000\n",
            "Trainable params: 1,521,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get 10 random numbers to choose random sentences and calculate BLEU score\n",
        "# note that code must be refactored: it was merged from examples and is \n",
        "# inconsistent now\n",
        "questions = train_df['Question'].to_list()\n",
        "rand_integers = [random.randint(0, len(questions)-1) for i in range(1, 10)]\n",
        "bleu_total = 0\n",
        "\n",
        "\n",
        "for i in rand_integers:\n",
        "    states_values = enc_model.predict(str_to_tokens(questions[i]))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    decoded_translation = decoded_translation[1:]\n",
        "\n",
        "    print(f'Original question: {questions[i]}')\n",
        "    print(f'Predicated answer: {decoded_translation}')\n",
        "\n",
        "    reference_answers = train_df.loc[train_df['Question']==questions[i], 'norm_answer'].to_list()\n",
        "    reference_answers = [answer[8:-7] for answer in reference_answers]\n",
        "\n",
        "\n",
        "    # The following should contain all possible answers, though...\n",
        "    print(f'{reference_answers}')\n",
        "    bleu_score = sentence_bleu(reference_answers, decoded_translation, smoothing_function=SmoothingFunction().method0)\n",
        "    print(f'Bleu score: {bleu_score}\\n')\n",
        "    bleu_total += bleu_score\n",
        "\n",
        "print(f'Bleu average = {bleu_total/len(rand_integers)}')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QObKQwyVLNzY",
        "outputId": "6722cc88-14cb-415b-dcd0-4eed25db7826"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original question: when does the electoral college votes\n",
            "Predicated answer: the president and vice president are not elected directly by the voters\n",
            "['electoral college map showing the results of the 2012 us presidential election ', 'the president and vice president are not elected directly by the voters', 'the number of electors in each state is equal to the number of members of congress to which the state is entitled']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: What country has a single solid color flag\n",
            "Predicated answer: there are three distinct types of national flag for use on land and three for use at sea though many countries use identical designs for several and sometimes all of these types of flag\n",
            "['there are three distinct types of national flag for use on land and three for use at sea though many countries use identical designs for several and sometimes all of these types of flag']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: what level does gears of war go up to\n",
            "Predicated answer: the game has since sold over five million copies by september 2008\n",
            "['it currently stands as the 5th best selling xbox 360 game of all time', 'the game has since sold over five million copies by september 2008']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: when was the Declaration of Independence  Signed?\n",
            "Predicated answer: the declaration was ultimately a formal explanation of why congress had voted on july 2 to declare independence from great britain more than a year after the outbreak of the american revolutionary war\n",
            "['the declaration of independence is a statement adopted by the continental congress on july 4 1776 which announced that the thirteen american colonies  then at war with great britain  regarded themselves as independent states and no longer a part of the british empire ', 'a committee had already drafted the formal declaration to be ready when congress voted on independence', 'the declaration was ultimately a formal explanation of why congress had voted on july 2 to declare independence from great britain more than a year after the outbreak of the american revolutionary war ', 'the national birthday the independence day is celebrated on july 4 although adams wanted july 2', 'since then it has come to be considered a major statement on human rights  particularly its second sentence']\n",
            "Bleu score: 0.9950124791926824\n",
            "\n",
            "Original question: who killed julius caesar\n",
            "Predicated answer: caesar became the first roman general to cross both when he built a bridge across the rhine and conducted the first invasion of britain\n",
            "['caesar became the first roman general to cross both when he built a bridge across the rhine and conducted the first invasion of britain ', 'a new series of civil wars broke out and the constitutional government of the republic was never restored']\n",
            "Bleu score: 0.9926199598197506\n",
            "\n",
            "Original question: where is kos from?\n",
            "Predicated answer: the album received positive reviews but sold relatively few copies\n",
            "['a musician as well as a producer kos has written and produced nearly every part of all four of his albums', 'the album received positive reviews but sold relatively few copies', ' was released in 2009']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: who was the first civilian to make a magnetic compass\n",
            "Predicated answer: the frame of reference defines the four cardinal directions or points â north south east and west\n",
            "['a simple dry magnetic portable compass', 'a military compass that was used during world war i ', 'the frame of reference defines the four cardinal directions or points â\\x80\\x93 north  south  east  and west ', 'usually a diagram called a compass rose  which shows the directions with their names usually abbreviated to initials is marked on the compass']\n",
            "Bleu score: 0.9401205263519623\n",
            "\n",
            "Original question: what causes thunder sound\n",
            "Predicated answer: thunder is the sound caused by lightning\n",
            "['thunder is the sound caused by lightning ']\n",
            "Bleu score: 0.9753099120283327\n",
            "\n",
            "Original question: what terminal is delta at lax\n",
            "Predicated answer: it is also the only airport to rank among the top five us airports for both passenger and cargo traffic\n",
            "['it is also the only airport to rank among the top five us airports for both passenger and cargo traffic']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Bleu average = 0.989229208599192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input('Ask me something, or enter \\'end\\' to stop: ')\n",
        "    if question == 'end':\n",
        "        break\n",
        "    states_values = enc_model.predict(str_to_tokens(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh",
        "outputId": "bfee5884-3be3-4e0d-f8a3-e0ed88cda5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ask me something, or enter 'end' to stop: what causes thunder sound\n",
            " thunder is the sound caused by lightning\n",
            "Ask me something, or enter 'end' to stop: why does thunder sound\n",
            " thunder is the sound caused by lightning\n",
            "Ask me something, or enter 'end' to stop: why is thunder loud\n",
            " thunder is the sound caused by lightning\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a219f8417ece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ask me something, or enter \\'end\\' to stop: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstates_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2ZYEEr1fFqUm"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}