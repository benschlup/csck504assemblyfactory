{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSCK507_Team_A_WikiQA_Chatbot_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/benschlup/csck507_team_a/blob/main/CSCK507_Team_A_ChatBot_THREE.ipynb",
      "authorship_tag": "ABX9TyOZ8DgF4xEvqa7meGChZFJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benschlup/csck504assemblyfactory/blob/main/CSCK507_Team_A_WikiQA_Chatbot_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### **CSCK507 Natural Language Processing, March-May 2022: End-of-Module Assignment**\n",
        "# **Generative Chatbot**\n",
        "---\n",
        "#### Team A\n",
        "Muhammad Ali (Student ID )  \n",
        "Benjamin Schlup (Student ID 200050007)  \n",
        "Chinedu Abonyi (Student ID )  \n",
        "Victor Armenta-Valdes (Student ID )\n",
        "\n",
        "---\n",
        "# **Solution 1: LSTM without Attention Layer**\n",
        "---"
      ],
      "metadata": {
        "id": "dXeItkpo51bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset being used: https://www.microsoft.com/en-us/download/details.aspx?id=52419  \n",
        "Paper on dataset: https://aclanthology.org/D15-1237/  \n",
        "Solution inspired by https://medium.com/swlh/how-to-design-seq2seq-chatbot-using-keras-framework-ae86d950e91d  \n",
        "\n",
        "Additional interesting materials to review, and potentially reference:\n",
        "Khin, N.N., Soe, K.M., 2020. Question Answering based University Chatbot using Sequence to Sequence Model, in: .. doi:10.1109/o-cocosda50338.2020.9295021\n",
        "\n"
      ],
      "metadata": {
        "id": "kv0kmUiLmJSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Backlog:\n",
        "* Use real validation set\n",
        "* Strip whitespace at beginning and end of questions and sentences\n",
        "* Check if lemmatizing on question side improves performance\n",
        "* Check if word embedding (e.g. using Word2Vec or GloVe) improves performance (beware of out-of-vocab)\n",
        "---"
      ],
      "metadata": {
        "id": "AHuZjEDChQ_J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Configuration"
      ],
      "metadata": {
        "id": "subk2_v1tjeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The dataset includes invalid answers (labelled 0) and some questions \n",
        "# even have no valid answer at all: Switches allow test runs excluding invalid\n",
        "# answers.\n",
        "# Note that the assignment says that answers must be provided by the chatbot: \n",
        "# there is no mention that answers must be correct!\n",
        "train_with_invalid_answers = True\n",
        "validate_with_invalid_answers = True\n",
        "test_questions_without_valid_answers = True\n",
        "\n",
        "# The dataset contains questions with multiple valid answers\n",
        "train_with_duplicate_questions = True\n",
        "validate_with_duplicate_questions = True\n",
        "test_with_duplicate_questions = True\n",
        "\n",
        "# Configure the tokenizer\n",
        "vocab_size_limit = 6000 + 1 # set this to None if all tokens from training shall be included (add one to number of tokens)\n",
        "vocab_include_val = False   # set this to True if tokens from validation set shall be included in vocabulary\n",
        "vocab_include_test = False  # set this to True if tokens from test set shall be included in vocabulary\n",
        "oov_token = 1               # set this to None if out-of-vocabulary tokens should be removed from sequences\n",
        "remove_oov_sentences = True # set this to True if any sentences containing out-of-vocabulary tokens should be removed from training, validation, test dataset\n",
        "\n",
        "# Limit sentence lengths // not yet implemented\n",
        "max_question_tokens = 20    # set this to None if no limit on question length\n",
        "max_answer_tokens = 50      # set this to None if no limit on answer length"
      ],
      "metadata": {
        "id": "_hFMwuk8td8V"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "Fq0L8soItfbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import codecs\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import urllib.request\n",
        "import yaml\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#from gensim.models import Word2Vec\n",
        "\n",
        "from tensorflow.keras.activations import softmax\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "CmdlY3dO1O_S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the GPU is visible to our runtime\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ],
      "metadata": {
        "id": "B9cNSuwm07wi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what GPU we have in place\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "ijf1uMKAXnbt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ebd7d3f-81ab-4b7b-9be8-eb724d113b6c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed May 11 17:18:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    40W / 250W |   2639MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data: If link does not work any longer, access file manually from here: https://www.microsoft.com/en-us/download/details.aspx?id=52419\n",
        "urllib.request.urlretrieve(\"https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\", \"WikiQACorpus.zip\")"
      ],
      "metadata": {
        "id": "mYkrBnyV1L-E",
        "outputId": "e94f31a2-b06c-4b45-a85d-3a75c7f4af66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('WikiQACorpus.zip', <http.client.HTTPMessage at 0x7f6f246dae90>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract files\n",
        "with zipfile.ZipFile('WikiQACorpus.zip', 'r') as zipfile:\n",
        "   zipfile.extractall()"
      ],
      "metadata": {
        "id": "d09_-PN51ois"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import questions and answers: training, validation and test datasets\n",
        "train_df = pd.read_csv( f'./WikiQACorpus/WikiQA-train.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "val_df = pd.read_csv( f'./WikiQACorpus/WikiQA-dev.tsv', sep='\\t', encoding='ISO-8859-1')\n",
        "test_df = pd.read_csv( f'./WikiQACorpus/WikiQA-test.tsv', sep='\\t', encoding='ISO-8859-1')       "
      ],
      "metadata": {
        "id": "e_tpDQAUEiKK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quality checks and exploratory data analysis removed: dataset has proven clean\n",
        "# Print gross volumes:\n",
        "print(f'Gross training dataset size: {len(train_df)}')\n",
        "print(f'Gross validation dataset size: {len(val_df)}')\n",
        "print(f'Gross test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPMMJHDhvRsN",
        "outputId": "c69c9548-bfb9-4d8d-d153-7d714370ae8c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gross training dataset size: 20347\n",
            "Gross validation dataset size: 2733\n",
            "Gross test dataset size: 6116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove q/a pairs depending on configuration of the notebook\n",
        "if not train_with_invalid_answers:\n",
        "    train_df = train_df[train_df['Label'] == 1]\n",
        "if not validate_with_invalid_answers:\n",
        "    val_df = val_df[val_df['Label'] == 1]\n",
        "if not test_questions_without_valid_answers:\n",
        "    test_df = test_df[test_df['Label'] == 1]"
      ],
      "metadata": {
        "id": "7kJkWGVMs5kJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate questions in case configured to do so\n",
        "if not train_with_duplicate_questions:\n",
        "    train_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not validate_with_duplicate_questions:\n",
        "    validate_df.drop_duplicates(subset=['Question'], inplace=True)\n",
        "if not test_with_duplicate_questions:\n",
        "    test_df.drop_duplicates(subset=['Question'], inplace=True)"
      ],
      "metadata": {
        "id": "6hf9fo1r0PdJ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Derive normalized questions and answers\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df.loc[:,'norm_question'] = [ re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", q).lower() for q in df['Question'] ]\n",
        "    df.loc[:,'norm_answer'] = [ '_START_ '+re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", s).lower()+' _STOP_' for s in df['Sentence']]"
      ],
      "metadata": {
        "id": "QQ1553hGYQL2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation:\n",
        "# Tokenization:\n",
        "# Reconsider adding digits to filter later, as encoding of numbers may create excessive vocabulary\n",
        "# Also check reference on handling numbers in NLP: https://arxiv.org/abs/2103.13136\n",
        "# Note that I do not yet train the tokenizer on validation and test datasets - should be challenged. \n",
        "# my be added to Tokenizer filters=target_regex = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\''\n",
        "\n",
        "if remove_oov_sentences:\n",
        "    oov_token = None\n",
        "tokenizer = Tokenizer(num_words=vocab_size_limit, oov_token=oov_token)\n",
        "\n",
        "tokenizer.fit_on_texts(train_df['norm_question'] + train_df['norm_answer'])\n",
        "if vocab_include_val:\n",
        "    tokenizer.fit_on_texts(val_df['norm_question'] + val_df['norm_answer'])\n",
        "if vocab_include_test:\n",
        "    tokenizer.fit_on_texts(test_df['norm_question'] + test_df['norm_answer'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "if vocab_size_limit is not None:\n",
        "    vocab_size = min([vocab_size, vocab_size_limit])\n",
        "print(f'Vocabulary size based on training dataset: {vocab_size}')\n",
        "\n",
        "for df in [train_df, val_df, test_df]:\n",
        "    df['tokenized_question'] = tokenizer.texts_to_sequences(df['norm_question'])\n",
        "    df['tokenized_answer'] = tokenizer.texts_to_sequences(df['norm_answer'])\n",
        "    df['question_tokens'] = [ len(x.split()) for x in df['norm_question'] ]\n",
        "    df['answer_tokens'] = [ len(x.split()) for x in df['norm_answer'] ]\n",
        "    if remove_oov_sentences:\n",
        "        df.drop(df[df['question_tokens']!=df['tokenized_question'].str.len()].index, inplace=True)\n",
        "        df.drop(df[df['answer_tokens']!=df['tokenized_answer'].str.len()].index, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZedlpHo6-62P",
        "outputId": "677a53df-20ca-410a-ec61-bd1743a28ea9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size based on training dataset: 6001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print net volumes\n",
        "print(f'Net training dataset size: {len(train_df)}')\n",
        "print(f'Net validation dataset size: {len(val_df)}')\n",
        "print(f'Net test dataset size: {len(test_df)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06d6dbac-d4be-4107-aa17-143f717a838e",
        "id": "LuYn2ANsxSAm"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net training dataset size: 2181\n",
            "Net validation dataset size: 108\n",
            "Net test dataset size: 252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "\n",
        "maxlen_questions = max(len(t) for t in train_df['tokenized_question'].to_list())\n",
        "maxlen_answers = max(len(t) for t in train_df['tokenized_answer'].to_list())\n",
        "\n",
        "train_encoder_input_data = pad_sequences(train_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "val_encoder_input_data = pad_sequences(val_df['tokenized_question'], maxlen=maxlen_questions, padding='post')\n",
        "print(f'Encoder input data shape: {train_encoder_input_data.shape}')\n",
        "\n",
        "train_decoder_input_data = pad_sequences(train_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_input_data = pad_sequences(val_df['tokenized_answer'], maxlen=maxlen_answers, padding='post')\n",
        "print(f'Decoder input data shape: {train_decoder_input_data.shape}')\n",
        "\n",
        "tokenized_answers = [ ta[1:] for ta in train_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "train_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "tokenized_answers = [ ta[1:] for ta in val_df['tokenized_answer'] ]\n",
        "padded_answers = pad_sequences(tokenized_answers, maxlen=maxlen_answers, padding='post')\n",
        "val_decoder_output_data = to_categorical(padded_answers, vocab_size)\n",
        "print(f'Decoder output data shape: {decoder_output_data.shape}')\n",
        "\n",
        "enc_inputs = Input(shape=(None,))\n",
        "enc_embedding = Embedding(vocab_size, 200, mask_zero=True)(enc_inputs)\n",
        "_, state_h, state_c = LSTM(200, return_state=True)(enc_embedding)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_embedding = Embedding(vocab_size, 200, mask_zero=True)(dec_inputs)\n",
        "dec_lstm = LSTM(200, return_state=True, return_sequences=True)\n",
        "dec_outputs, _, _ = dec_lstm(dec_embedding, initial_state=enc_states)\n",
        "dec_dense = Dense(vocab_size, activation=softmax)\n",
        "output = dec_dense(dec_outputs)\n",
        "\n",
        "model = Model([enc_inputs, dec_inputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzFFnaCE5TIe",
        "outputId": "74ce8e3b-5dd9-4be8-98e4-4c491314d905"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder input data shape: (2181, 21)\n",
            "Decoder input data shape: (2181, 52)\n",
            "Decoder output data shape: (2181, 52, 6001)\n",
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_11 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, None, 200)    1200200     ['input_11[0][0]']               \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, None, 200)    1200200     ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " lstm_6 (LSTM)                  [(None, 200),        320800      ['embedding_6[0][0]']            \n",
            "                                 (None, 200),                                                     \n",
            "                                 (None, 200)]                                                     \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 200),  320800      ['embedding_7[0][0]',            \n",
            "                                 (None, 200),                     'lstm_6[0][1]',                 \n",
            "                                 (None, 200)]                     'lstm_6[0][2]']                 \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, None, 6001)   1206201     ['lstm_7[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4,248,201\n",
            "Trainable params: 4,248,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model training\n",
        "\n",
        "model.fit([train_encoder_input_data, train_decoder_input_data], train_decoder_output_data,\n",
        "          validation_data=([val_encoder_input_data, val_decoder_input_data], val_decoder_output_data),\n",
        "          batch_size=50, epochs=200)\n",
        "\n",
        "#model.save('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glC5E6w1M9mk",
        "outputId": "f11402f1-f5b8-42bd-b2a0-c0f64394806b"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "44/44 [==============================] - 2s 40ms/step - loss: 0.7672 - val_loss: 1.7035\n",
            "Epoch 2/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.7444 - val_loss: 1.7045\n",
            "Epoch 3/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.7214 - val_loss: 1.7132\n",
            "Epoch 4/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.6993 - val_loss: 1.7149\n",
            "Epoch 5/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6775 - val_loss: 1.7094\n",
            "Epoch 6/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6559 - val_loss: 1.7163\n",
            "Epoch 7/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6345 - val_loss: 1.7239\n",
            "Epoch 8/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.6146 - val_loss: 1.7233\n",
            "Epoch 9/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5944 - val_loss: 1.7198\n",
            "Epoch 10/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.5743 - val_loss: 1.7284\n",
            "Epoch 11/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5552 - val_loss: 1.7315\n",
            "Epoch 12/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5366 - val_loss: 1.7394\n",
            "Epoch 13/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.5184 - val_loss: 1.7472\n",
            "Epoch 14/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.5009 - val_loss: 1.7524\n",
            "Epoch 15/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4833 - val_loss: 1.7510\n",
            "Epoch 16/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4659 - val_loss: 1.7583\n",
            "Epoch 17/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.4496 - val_loss: 1.7510\n",
            "Epoch 18/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.4336 - val_loss: 1.7688\n",
            "Epoch 19/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4180 - val_loss: 1.7678\n",
            "Epoch 20/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.4024 - val_loss: 1.7901\n",
            "Epoch 21/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.3882 - val_loss: 1.7834\n",
            "Epoch 22/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.3734 - val_loss: 1.7889\n",
            "Epoch 23/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3589 - val_loss: 1.7883\n",
            "Epoch 24/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3462 - val_loss: 1.7860\n",
            "Epoch 25/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3317 - val_loss: 1.7964\n",
            "Epoch 26/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3198 - val_loss: 1.8074\n",
            "Epoch 27/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.3067 - val_loss: 1.8092\n",
            "Epoch 28/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2953 - val_loss: 1.8245\n",
            "Epoch 29/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.2833 - val_loss: 1.8231\n",
            "Epoch 30/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2717 - val_loss: 1.8353\n",
            "Epoch 31/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.2610 - val_loss: 1.8415\n",
            "Epoch 32/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2510 - val_loss: 1.8431\n",
            "Epoch 33/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.2400 - val_loss: 1.8501\n",
            "Epoch 34/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2305 - val_loss: 1.8610\n",
            "Epoch 35/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.2208 - val_loss: 1.8782\n",
            "Epoch 36/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.2117 - val_loss: 1.8758\n",
            "Epoch 37/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.2027 - val_loss: 1.8687\n",
            "Epoch 38/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.1943 - val_loss: 1.8924\n",
            "Epoch 39/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.1862 - val_loss: 1.8918\n",
            "Epoch 40/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.1788 - val_loss: 1.8989\n",
            "Epoch 41/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1708 - val_loss: 1.9006\n",
            "Epoch 42/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1632 - val_loss: 1.9007\n",
            "Epoch 43/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.1569 - val_loss: 1.9134\n",
            "Epoch 44/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1500 - val_loss: 1.9304\n",
            "Epoch 45/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1438 - val_loss: 1.9282\n",
            "Epoch 46/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1376 - val_loss: 1.9344\n",
            "Epoch 47/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1313 - val_loss: 1.9493\n",
            "Epoch 48/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.1258 - val_loss: 1.9478\n",
            "Epoch 49/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1209 - val_loss: 1.9535\n",
            "Epoch 50/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1153 - val_loss: 1.9704\n",
            "Epoch 51/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1103 - val_loss: 1.9778\n",
            "Epoch 52/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1064 - val_loss: 1.9799\n",
            "Epoch 53/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.1014 - val_loss: 1.9885\n",
            "Epoch 54/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0974 - val_loss: 1.9954\n",
            "Epoch 55/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0942 - val_loss: 1.9902\n",
            "Epoch 56/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0897 - val_loss: 2.0071\n",
            "Epoch 57/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0863 - val_loss: 2.0071\n",
            "Epoch 58/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0827 - val_loss: 2.0076\n",
            "Epoch 59/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0793 - val_loss: 2.0247\n",
            "Epoch 60/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0766 - val_loss: 2.0269\n",
            "Epoch 61/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0734 - val_loss: 2.0435\n",
            "Epoch 62/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0708 - val_loss: 2.0388\n",
            "Epoch 63/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0684 - val_loss: 2.0421\n",
            "Epoch 64/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0657 - val_loss: 2.0619\n",
            "Epoch 65/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0638 - val_loss: 2.0624\n",
            "Epoch 66/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0617 - val_loss: 2.0670\n",
            "Epoch 67/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0596 - val_loss: 2.0593\n",
            "Epoch 68/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0574 - val_loss: 2.0831\n",
            "Epoch 69/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0557 - val_loss: 2.0656\n",
            "Epoch 70/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0542 - val_loss: 2.1030\n",
            "Epoch 71/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0524 - val_loss: 2.0764\n",
            "Epoch 72/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0511 - val_loss: 2.1102\n",
            "Epoch 73/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0496 - val_loss: 2.1106\n",
            "Epoch 74/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0484 - val_loss: 2.1091\n",
            "Epoch 75/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0472 - val_loss: 2.1079\n",
            "Epoch 76/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0462 - val_loss: 2.1418\n",
            "Epoch 77/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0450 - val_loss: 2.1301\n",
            "Epoch 78/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0440 - val_loss: 2.1205\n",
            "Epoch 79/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0431 - val_loss: 2.1431\n",
            "Epoch 80/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0422 - val_loss: 2.1347\n",
            "Epoch 81/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0415 - val_loss: 2.1479\n",
            "Epoch 82/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0407 - val_loss: 2.1555\n",
            "Epoch 83/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0398 - val_loss: 2.1476\n",
            "Epoch 84/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0392 - val_loss: 2.1532\n",
            "Epoch 85/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0388 - val_loss: 2.1601\n",
            "Epoch 86/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0379 - val_loss: 2.1534\n",
            "Epoch 87/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0373 - val_loss: 2.1800\n",
            "Epoch 88/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0369 - val_loss: 2.2044\n",
            "Epoch 89/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0364 - val_loss: 2.1935\n",
            "Epoch 90/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0358 - val_loss: 2.1948\n",
            "Epoch 91/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0354 - val_loss: 2.1941\n",
            "Epoch 92/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0349 - val_loss: 2.1953\n",
            "Epoch 93/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0346 - val_loss: 2.2049\n",
            "Epoch 94/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0345 - val_loss: 2.2127\n",
            "Epoch 95/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0338 - val_loss: 2.2182\n",
            "Epoch 96/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0337 - val_loss: 2.2122\n",
            "Epoch 97/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0332 - val_loss: 2.2124\n",
            "Epoch 98/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0330 - val_loss: 2.2217\n",
            "Epoch 99/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0327 - val_loss: 2.2425\n",
            "Epoch 100/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0324 - val_loss: 2.2373\n",
            "Epoch 101/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0322 - val_loss: 2.2386\n",
            "Epoch 102/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0319 - val_loss: 2.2399\n",
            "Epoch 103/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0316 - val_loss: 2.2437\n",
            "Epoch 104/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0313 - val_loss: 2.2587\n",
            "Epoch 105/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0311 - val_loss: 2.2385\n",
            "Epoch 106/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0310 - val_loss: 2.2557\n",
            "Epoch 107/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0307 - val_loss: 2.2546\n",
            "Epoch 108/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0305 - val_loss: 2.2796\n",
            "Epoch 109/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0304 - val_loss: 2.2686\n",
            "Epoch 110/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0303 - val_loss: 2.2897\n",
            "Epoch 111/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0301 - val_loss: 2.2987\n",
            "Epoch 112/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0301 - val_loss: 2.2869\n",
            "Epoch 113/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0299 - val_loss: 2.2869\n",
            "Epoch 114/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0296 - val_loss: 2.2958\n",
            "Epoch 115/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0294 - val_loss: 2.2959\n",
            "Epoch 116/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0295 - val_loss: 2.3074\n",
            "Epoch 117/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0292 - val_loss: 2.2980\n",
            "Epoch 118/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0293 - val_loss: 2.3220\n",
            "Epoch 119/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0291 - val_loss: 2.3090\n",
            "Epoch 120/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0290 - val_loss: 2.3194\n",
            "Epoch 121/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0289 - val_loss: 2.3128\n",
            "Epoch 122/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0286 - val_loss: 2.3161\n",
            "Epoch 123/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0287 - val_loss: 2.3295\n",
            "Epoch 124/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0287 - val_loss: 2.3306\n",
            "Epoch 125/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0285 - val_loss: 2.3287\n",
            "Epoch 126/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0285 - val_loss: 2.3455\n",
            "Epoch 127/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0284 - val_loss: 2.3560\n",
            "Epoch 128/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0283 - val_loss: 2.3515\n",
            "Epoch 129/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0282 - val_loss: 2.3385\n",
            "Epoch 130/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0281 - val_loss: 2.3582\n",
            "Epoch 131/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0280 - val_loss: 2.3596\n",
            "Epoch 132/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0279 - val_loss: 2.3644\n",
            "Epoch 133/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0279 - val_loss: 2.3638\n",
            "Epoch 134/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0279 - val_loss: 2.3887\n",
            "Epoch 135/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0277 - val_loss: 2.3847\n",
            "Epoch 136/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0277 - val_loss: 2.3863\n",
            "Epoch 137/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0276 - val_loss: 2.3904\n",
            "Epoch 138/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0276 - val_loss: 2.3918\n",
            "Epoch 139/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0275 - val_loss: 2.3959\n",
            "Epoch 140/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0275 - val_loss: 2.3962\n",
            "Epoch 141/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0274 - val_loss: 2.4105\n",
            "Epoch 142/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0274 - val_loss: 2.4077\n",
            "Epoch 143/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0274 - val_loss: 2.4078\n",
            "Epoch 144/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0274 - val_loss: 2.4052\n",
            "Epoch 145/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0273 - val_loss: 2.4242\n",
            "Epoch 146/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0271 - val_loss: 2.4189\n",
            "Epoch 147/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0272 - val_loss: 2.4449\n",
            "Epoch 148/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0270 - val_loss: 2.4225\n",
            "Epoch 149/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0270 - val_loss: 2.4413\n",
            "Epoch 150/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0270 - val_loss: 2.4296\n",
            "Epoch 151/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0269 - val_loss: 2.4386\n",
            "Epoch 152/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0269 - val_loss: 2.4333\n",
            "Epoch 153/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0269 - val_loss: 2.4514\n",
            "Epoch 154/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0268 - val_loss: 2.4420\n",
            "Epoch 155/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0268 - val_loss: 2.4410\n",
            "Epoch 156/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0267 - val_loss: 2.4645\n",
            "Epoch 157/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0267 - val_loss: 2.4683\n",
            "Epoch 158/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0267 - val_loss: 2.4665\n",
            "Epoch 159/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0266 - val_loss: 2.4701\n",
            "Epoch 160/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0266 - val_loss: 2.4666\n",
            "Epoch 161/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0266 - val_loss: 2.4738\n",
            "Epoch 162/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0265 - val_loss: 2.4791\n",
            "Epoch 163/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0265 - val_loss: 2.4876\n",
            "Epoch 164/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0267 - val_loss: 2.4863\n",
            "Epoch 165/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0263 - val_loss: 2.4811\n",
            "Epoch 166/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0264 - val_loss: 2.4799\n",
            "Epoch 167/200\n",
            "44/44 [==============================] - 2s 35ms/step - loss: 0.0263 - val_loss: 2.4897\n",
            "Epoch 168/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0265 - val_loss: 2.4994\n",
            "Epoch 169/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0263 - val_loss: 2.5030\n",
            "Epoch 170/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0263 - val_loss: 2.4831\n",
            "Epoch 171/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0262 - val_loss: 2.5114\n",
            "Epoch 172/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0263 - val_loss: 2.4948\n",
            "Epoch 173/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0263 - val_loss: 2.5178\n",
            "Epoch 174/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0262 - val_loss: 2.5188\n",
            "Epoch 175/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0261 - val_loss: 2.5208\n",
            "Epoch 176/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0261 - val_loss: 2.5357\n",
            "Epoch 177/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0261 - val_loss: 2.5368\n",
            "Epoch 178/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0261 - val_loss: 2.5232\n",
            "Epoch 179/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0262 - val_loss: 2.5308\n",
            "Epoch 180/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0260 - val_loss: 2.5166\n",
            "Epoch 181/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0260 - val_loss: 2.5462\n",
            "Epoch 182/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0260 - val_loss: 2.5432\n",
            "Epoch 183/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0260 - val_loss: 2.5402\n",
            "Epoch 184/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0261 - val_loss: 2.5534\n",
            "Epoch 185/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0258 - val_loss: 2.5431\n",
            "Epoch 186/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0261 - val_loss: 2.5596\n",
            "Epoch 187/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0260 - val_loss: 2.5416\n",
            "Epoch 188/200\n",
            "44/44 [==============================] - 2s 38ms/step - loss: 0.0260 - val_loss: 2.5647\n",
            "Epoch 189/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0258 - val_loss: 2.5838\n",
            "Epoch 190/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0259 - val_loss: 2.5594\n",
            "Epoch 191/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0259 - val_loss: 2.5715\n",
            "Epoch 192/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0258 - val_loss: 2.5770\n",
            "Epoch 193/200\n",
            "44/44 [==============================] - 2s 37ms/step - loss: 0.0259 - val_loss: 2.5766\n",
            "Epoch 194/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0258 - val_loss: 2.5671\n",
            "Epoch 195/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0258 - val_loss: 2.5799\n",
            "Epoch 196/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0257 - val_loss: 2.5708\n",
            "Epoch 197/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0257 - val_loss: 2.5823\n",
            "Epoch 198/200\n",
            "44/44 [==============================] - 2s 34ms/step - loss: 0.0258 - val_loss: 2.5799\n",
            "Epoch 199/200\n",
            "44/44 [==============================] - 2s 35ms/step - loss: 0.0257 - val_loss: 2.5946\n",
            "Epoch 200/200\n",
            "44/44 [==============================] - 2s 36ms/step - loss: 0.0258 - val_loss: 2.5810\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6e66cfb9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare models for inferencing (separate encoder, decoder)\n",
        "#model.load_weights('/content/drive/MyDrive/CSCK507_Team_A/qa_model.h5')\n",
        "\n",
        "def make_inference_models():\n",
        "    dec_state_input_h = Input(shape=(200,))\n",
        "    dec_state_input_c = Input(shape=(200,))\n",
        "    dec_states_inputs = [dec_state_input_h, dec_state_input_c]\n",
        "    dec_outputs, state_h, state_c = dec_lstm(dec_embedding,\n",
        "                                             initial_state=dec_states_inputs)\n",
        "    dec_states = [state_h, state_c]\n",
        "    dec_outputs = dec_dense(dec_outputs)\n",
        "\n",
        "    dec_model = Model(\n",
        "        inputs=[dec_inputs] + dec_states_inputs,\n",
        "        outputs=[dec_outputs] + dec_states)\n",
        "    print('Inference decoder:')\n",
        "    dec_model.summary()\n",
        "\n",
        "    enc_model = Model(inputs=enc_inputs, outputs=enc_states)\n",
        "    print('Inference encoder:')\n",
        "    enc_model.summary()\n",
        "    return enc_model, dec_model\n",
        "\n",
        "def str_to_tokens(sentence):\n",
        "    words = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", sentence).lower().split()\n",
        "    tokens_list = list()\n",
        "    for current_word in words:\n",
        "        result = tokenizer.word_index.get(current_word, '')\n",
        "        if result != '':\n",
        "            tokens_list.append(result)\n",
        "        else:\n",
        "            print(f'Warning: out-of-vocabulary token \\'{current_word}\\'')\n",
        "            if oov_token is not None:\n",
        "                tokens_list.append(oov_token)\n",
        "\n",
        "    return pad_sequences([tokens_list],\n",
        "                         maxlen=maxlen_questions,\n",
        "                         padding='post')\n",
        "\n",
        "\n",
        "enc_model, dec_model = make_inference_models()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHYIs3pL86Ov",
        "outputId": "a36008e2-2a78-4793-8176-a4cf87ae6384"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference decoder:\n",
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding_7 (Embedding)        (None, None, 200)    1200200     ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " input_23 (InputLayer)          [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " input_24 (InputLayer)          [(None, 200)]        0           []                               \n",
            "                                                                                                  \n",
            " lstm_7 (LSTM)                  [(None, None, 200),  320800      ['embedding_7[0][0]',            \n",
            "                                 (None, 200),                     'input_23[0][0]',               \n",
            "                                 (None, 200)]                     'input_24[0][0]']               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, None, 6001)   1206201     ['lstm_7[6][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,727,201\n",
            "Trainable params: 2,727,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Inference encoder:\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, None, 200)         1200200   \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               [(None, 200),             320800    \n",
            "                              (None, 200),                       \n",
            "                              (None, 200)]                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,521,000\n",
            "Trainable params: 1,521,000\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get 10 random numbers to choose random sentences and calculate BLEU score\n",
        "# note that code must be refactored: it was merged from examples and is \n",
        "# inconsistent now\n",
        "questions = train_df['Question'].to_list()\n",
        "rand_integers = [random.randint(0, len(questions)-1) for i in range(1, 10)]\n",
        "bleu_total = 0\n",
        "\n",
        "\n",
        "for i in rand_integers:\n",
        "    states_values = enc_model.predict(str_to_tokens(questions[i]))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    decoded_translation = decoded_translation[1:]\n",
        "\n",
        "    print(f'Original question: {questions[i]}')\n",
        "    print(f'Predicated answer: {decoded_translation}')\n",
        "\n",
        "    reference_answers = train_df.loc[train_df['Question']==questions[i], 'norm_answer'].to_list()\n",
        "    reference_answers = [answer[8:-7] for answer in reference_answers]\n",
        "\n",
        "\n",
        "    # The following should contain all possible answers, though...\n",
        "    print(f'{reference_answers}')\n",
        "    bleu_score = sentence_bleu(reference_answers, decoded_translation, smoothing_function=SmoothingFunction().method0)\n",
        "    print(f'Bleu score: {bleu_score}\\n')\n",
        "    bleu_total += bleu_score\n",
        "\n",
        "print(f'Bleu average = {bleu_total/len(rand_integers)}')\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QObKQwyVLNzY",
        "outputId": "ca7f0b4c-5ce3-424d-8453-4a20ddc0cffc"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original question: what is RFID equipment\n",
            "Predicated answer: rfid tags are used in many industries\n",
            "['rfid chip next to a grain of rice', 'battery powered tags may operate at hundreds of meters', 'rfid tags are used in many industries', 'an rfid tag attached to an automobile during production can be used to track its progress through the assembly line']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: how is paper measured\n",
            "Predicated answer: the term density is not used in its traditional sense of mass per unit volume\n",
            "['the term density is not used in its traditional sense of mass per unit volume ', 'paper density rather is a measure of the area density ', 'this is the measure used in most parts of the world', 'expressed in terms of the mass expressed as weight per number of sheets it is known as basis weight']\n",
            "Bleu score: 0.9870969543780109\n",
            "\n",
            "Original question: what is the capital city of california.\n",
            "Predicated answer: in addition the university of california davis is located in nearby davis west of the capital\n",
            "['sacramento is the capital city of the us state of california and the seat of government of sacramento county ', 'in addition the university of california davis is located in nearby davis  west of the capital']\n",
            "Bleu score: 0.9810485197284439\n",
            "\n",
            "Original question: what state was the civil war in\n",
            "Predicated answer: the american civil war was one of the earliest true industrial wars\n",
            "['the war had its origin in the issue of slavery  especially the extension of slavery into the western territories', 'foreign powers did not intervene', \"in the 1860 presidential election  republicans led by abraham lincoln  opposed expanding slavery into united states' territories \", 'eight remaining slave states continued to reject calls for secession', 'a peace conference failed to find a compromise and both sides prepared for war', 'lincoln issued the emancipation proclamation  which made ending slavery a war goal', 'the american civil war was one of the earliest true industrial wars ']\n",
            "Bleu score: 0.9851854581626466\n",
            "\n",
            "Original question: how many teams are in the world cup\n",
            "Predicated answer: the championship has been awarded every four years since the inaugural tournament in 1930 except in 1942 and 1946 when it was not held because of the second world war\n",
            "['the championship has been awarded every four years since the inaugural tournament in 1930  except in 1942 and 1946 when it was not held because of the second world war ', 'the current champions are spain  who won the 2010 tournament ', 'brazil have won five times and they are the only team to have played in every tournament']\n",
            "Bleu score: 0.983474363117061\n",
            "\n",
            "Original question: what is a base SI unit\n",
            "Predicated answer: these si base units and their physical quantities are\n",
            "['the international system of units si defines seven units of measure as a basic set from which all other si units are derived ', 'these si base units and their physical quantities are', 'second for time']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: when was the first automobile\n",
            "Predicated answer: world map of passenger cars per 1000 people\n",
            "['vehicles in use per country from 2001 to 2007', 'world map of passenger cars per 1000 people', 'the numbers are increasing rapidly especially in china and india ']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: what a wonderful world covers\n",
            "Predicated answer: it was first recorded by louis armstrong and released as a single in 1967\n",
            "['it was first recorded by louis armstrong and released as a single in 1967']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Original question: who is the actor that plays harry potter\n",
            "Predicated answer: thirteen actors have appeared as the same character in all eight films of the franchise\n",
            "['thirteen actors have appeared as the same character in all eight films of the franchise', 'key', 'v indicates the actor or actress lent only his or her voice for his or her film character']\n",
            "Bleu score: 1.0\n",
            "\n",
            "Bleu average = 0.9929783661540179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input('Ask me something, or enter \\'end\\' to stop: ')\n",
        "    if question == 'end':\n",
        "        break\n",
        "    states_values = enc_model.predict(str_to_tokens(question))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "\n",
        "    decoded_translation = ''\n",
        "    while True:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq]\n",
        "                                              + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if sampled_word_index == index:\n",
        "                if word != 'stop':\n",
        "                    decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'stop' \\\n",
        "                or len(decoded_translation.split()) \\\n",
        "                > maxlen_answers:\n",
        "            break\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    print(decoded_translation)"
      ],
      "metadata": {
        "id": "KAsbo2TRkAsh",
        "outputId": "01173e2e-158b-413f-93b6-88a376379aef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask me something, or enter 'end' to stop: this is asdf blue\n",
            "Warning: out-of-vocabulary token 'asdf'\n",
            " the 20th century most communist states service has been observed in the world and of the modern western league\n",
            "Ask me something, or enter 'end' to stop: when is it time for kennedy\n",
            " it is the only fashion designer to appear by number of the first lake nfl of the modern baseball\n",
            "Ask me something, or enter 'end' to stop: end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2ZYEEr1fFqUm"
      },
      "execution_count": 44,
      "outputs": []
    }
  ]
}